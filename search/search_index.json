{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"deploying/","title":"Deploying the RAG pattern","text":""},{"location":"deploying/#overview","title":"Overview","text":"<p>This project uses the validated patterns operator, an opinionated gitops system, to deploy onto an OpenShift cluster.</p>"},{"location":"deploying/#assumptions","title":"Assumptions","text":""},{"location":"deploying/#gpus","title":"GPUs","text":"<p>The current demonstration relies on <code>flash-attention</code> to decrease memory consumption for the LLM models. Today support to this limited to specific Nvidia GPUs which this system can work with. GPUs which are known to be good include:</p> <ul> <li>Nvidia L40S</li> <li>Nvidia A100</li> <li>Nvidia H100/H200</li> </ul> <p>Note: The V100 GPUs are not supported.</p>"},{"location":"deploying/#gpu-pool-management-wip","title":"GPU pool management (WIP)","text":"<p>The pattern today allows GPU pools to be managed for scale-out computing via MCAD and Instascale. It is important to note that this is designed primarily to manage scaling for batch workloads.</p> <p>This works where:</p> <ol> <li>The cluster auto-scaler is enabled (e.g. using the assisted installer into your own tenancy on AWS / GCP)</li> <li>Clusters managed via OpenShift Cluster Manager (e.g. ROSA, ARO and OSD)</li> </ol>"},{"location":"deploying/#bootstrap-machine","title":"Bootstrap machine","text":"<p><code>git</code> and <code>podman</code> are required on the bootstrap machine. A valid <code>KUBECONFIG</code> is required making <code>oc</code> (recommended) or <code>kubectl</code> strongly recommended.</p>"},{"location":"deploying/#setup-workflow","title":"Setup workflow","text":"<ol> <li> <p>Start <code>podman</code></p> </li> <li> <p>Fork the git repository</p> </li> <li> <p>Forking is required in order to customise the configuration.</p> </li> <li> <p>It is significantly easier to start with a public repository.</p> </li> <li> <p>Clone the forked git repository.</p> </li> <li> <p>Customise the <code>values-global.yaml</code> (see below)</p> </li> <li> <p><code>cp ./values-secret.yaml.template ~/values-secret-gen-llm-rag-pattern.yaml</code> and fill in required secrets</p> </li> <li> <p><code>cd</code> into the repository and <code>./pattern.sh make install</code></p> </li> </ol>"},{"location":"deploying/#required-configuration-in-order-to-function","title":"REQUIRED configuration in order to function","text":""},{"location":"deploying/#multicloud-object-gateway","title":"Multicloud Object Gateway","text":""},{"location":"dev-scripts/","title":"Development scripts","text":"<p>These scripts are useful for development and automation where the gap has not been completely closed.</p>"},{"location":"dev-scripts/#argo-envsh","title":"<code>argo-env.sh</code>","text":"<p>Two argoCD deployments are created by the validated patterns operator. The depending on your identity and RBAC setup you may not get access with <code>cluster-admin</code> or similar.</p> <p>Running (pre-authenticated with <code>oc</code>) <code>sh argo-env.sh</code> will provide the default admin passwords for each argo instance.</p>"},{"location":"developers/","title":"Developers","text":"<p>This page contains essential information for developers who are contributing to this pattern.</p>"},{"location":"developers/#pre-commit","title":"pre-commit","text":"<p>This project uses <code>pre-commit</code> as a mechanism to ensure code quality. <code>pre-commit</code> is enforced on pull requests as part of the CI pipeline. Installing pre-commit into a developers environment ensures that the quality checks will pass before code is pushed to GitHub.</p> <p>Install by:</p> <pre><code>pip install pre-commit\npre-commit install \n</code></pre> <p>developers can force <code>pre-commit</code> to run across all files by running <code>pre-commit run --all-files</code>. This will ensure every file is covered, including those which have already being committed. Standard runs will only check files with a change in git.</p>"},{"location":"developers/#documentation-continuous-integration-checks","title":"Documentation continuous integration checks","text":"<p>As part of the GitHub actions CICD projects there are a number of checks which inspect Markdown documents.</p> <p>Validation can fail for a number of reasons:</p> <ol> <li>Spelling mistakes including use of colloquial terms e.g. <code>repo</code> versus <code>repository</code> by the <code>superlinter</code></li> <li>Markdown document structure, enforced by the <code>pre-commit</code> and by the <code>superlinter</code></li> <li>Unlinked or dead links in the documentation site.</li> </ol>"},{"location":"developers/#dealing-with-unexpected-spelling-mistakes","title":"Dealing with unexpected spelling 'mistakes'","text":"<p>If a false-positive occurs for spelling mistakes:</p> <ol> <li>Add the word to <code>known-words.txt</code> or;</li> <li>If appropriate use inline <code>code</code> blocks to escape the word.</li> </ol>"},{"location":"user-stories/","title":"User stories","text":""},{"location":"user-stories/#motivation","title":"Motivation","text":"<p>The intent of these user stories is to provide a north star for this project.</p>"},{"location":"user-stories/#actors","title":"Actors","text":"<ul> <li> <p>Application Developer:</p> </li> <li> <p>Business Application Owner:</p> </li> </ul> The Business Application Owner is concerned with the business value the LLM delivers. <ul> <li>Data Engineer:</li> </ul> The Data Engineer actor is responsible for accessing, importing, and preparing data that is used by the LLM. <ul> <li> <p>Data scientist:</p> </li> <li> <p>Platform engineer:</p> </li> <li> <p>Security architect:</p> </li> <li> <p>Security engineer:</p> </li> </ul>"},{"location":"user-stories/#user-stories","title":"User stories","text":"<ul> <li> <p>As an Application Developer I want to be able to serve (multiple) LLM applications from my platform.</p> </li> <li> <p>As Business Application Owner I want my LLM to be up to date with my systems of record.</p> </li> <li> <p>As a Data Engineer I need to manage the lifecycle of data surrounding an LLM application, including: Ingestion &amp; preparation; building training datasets; Collecting QA data.</p> </li> <li> <p>As a Data Scientist I want to be able to produce an optimised LLM model to support the business use case.</p> </li> <li> <p>As a Data Scientist I want ready access to the latest opensource LLM innovations without getting in trouble with security.</p> </li> <li> <p>As a Platform Engineer I want to provide a platform enabling self-service for LLM development from experiment to production, while maintaining efficient use of the infrastructure (as a service) investment</p> </li> <li> <p>As a Security Architect I want LLM development initiatives to comply with my enterprise-wide security policies and controls.</p> </li> <li> <p>As a Security Architect I want to be able to measure and enforce that all software, models, and data come from trusted sources.</p> </li> <li> <p>As a SOC analyst I want sufficient capabilities to detect, investigate and correct security related events as they apply to the LLM platform and workloads</p> </li> </ul>"},{"location":"ADR/","title":"Architectural Decision Records (ADRs)","text":"<p>This project uses Compliance Trestle to generate and enforce the structure of ADRs. Read more about how <code>trestle author</code> is used to enforce template structure here</p> <p>In order to generate a new ADR:</p> <ol> <li><code>cd {REPO_ROOT}/docs</code></li> <li>Use <code>trestle author docs create-sample -tn ADR</code> to generate a new ADR document.</li> <li>Write your ADR with the created documentation.</li> <li>Validate the ADR with <code>trestle author docs validate -tn ADR -hv</code> from <code>{REPO_ROOT}/docs</code></li> <li>This is also completed as part of the test pipeline</li> <li>Commit and run through the standard PR review process.</li> </ol>"},{"location":"ADR/ADR_000/","title":"This title can be changed, please make it of the form <code>ADR 000: title</code>","text":"<p>All other headings below cannot be changed. This template is intended to be a <code>KISS</code> template to not burden the end user.</p>"},{"location":"ADR/ADR_000/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Write about the context and problem statement.</p>"},{"location":"ADR/ADR_000/#considered-options","title":"Considered Options","text":"<p>Multiple options should be considered.</p>"},{"location":"ADR/ADR_000/#decision-outcome","title":"Decision Outcome","text":"<p>Document the outcome.</p>"}]}